{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from src.SNMF import SNMF, update_code_within_radius\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from sklearn.metrics import roc_curve\n",
    "from scipy.spatial import ConvexHull\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import SparseCoder\n",
    "from pneumonia_dataprocess import process_path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"Data/fake_job_postings_v9.csv\"\n",
    "data1 = pd.read_csv(path1, delimiter=',')\n",
    "\n",
    "path2 = \"Data/results_data_description2.csv\"\n",
    "data2 = pd.read_csv(path2, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = data1['fraud']\n",
    "Y = np.asarray(d1) # indicator of fraud postings \n",
    "Y = Y[np.newaxis,:]\n",
    "print('Y.shape', Y.shape)\n",
    "print('number of fraud postings:', np.sum(Y))\n",
    "print('ratio of fraud postings:', np.sum(Y)/Y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = data1.get(data1.keys()[1:73]) # covariates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = data1.get(data1.keys()[73:]) # word frequencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = d3.values\n",
    "X = data2.values\n",
    "X = X - np.min(X) # word frequency array\n",
    "X = X.T\n",
    "print('X.shape', X.shape) # words x docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X.T, Y.T, test_size=0.2)\n",
    "X_train, X_test = X_train.T, X_test.T\n",
    "Y_train, Y_test = Y_train.T, Y_test.T\n",
    "print('X_train.shape', X_train.shape)\n",
    "print('Y_train.shape', Y_train.shape)\n",
    "print('X_test.shape', X_test.shape)\n",
    "print('Y_test.shape', Y_test.shape)\n",
    "print('number of fraud postings in Y_test:', np.sum(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx2word = data.keys()[73:]\n",
    "idx2word = data2.keys()\n",
    "print('idx2word', idx2word.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_metrics(Y_test, P_pred, use_opt_threshold=False, verbose=False):\n",
    "    # y_test = binary label\n",
    "    # P_pred = predicted probability for y_test\n",
    "    # compuate various binary classification accuracy metrics\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(Y_test, P_pred, pos_label=None)\n",
    "    mythre = thresholds[np.argmax(tpr - fpr)]\n",
    "    myauc = metrics.auc(fpr, tpr)\n",
    "    # print('!!! auc', myauc)\n",
    "\n",
    "    # Compute classification statistics\n",
    "    threshold = 0.5\n",
    "    if use_opt_threshold:\n",
    "        threshold = mythre\n",
    "\n",
    "    Y_pred = P_pred.copy()\n",
    "    Y_pred[Y_pred < threshold] = 0\n",
    "    Y_pred[Y_pred >= threshold] = 1\n",
    "\n",
    "    mcm = confusion_matrix(Y_test, Y_pred)\n",
    "    tn = mcm[0, 0]\n",
    "    tp = mcm[1, 1]\n",
    "    fn = mcm[1, 0]\n",
    "    fp = mcm[0, 1]\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    fall_out = fp / (fp + tn)\n",
    "    miss_rate = fn / (fn + tp)\n",
    "    recall = tp / (tp + fn)\n",
    "    F_score = 2 * precision * recall / ( precision + recall )\n",
    "\n",
    "    # Save results\n",
    "    results_dict = {}\n",
    "    results_dict.update({'Y_test': Y_test})\n",
    "    results_dict.update({'Y_pred': Y_pred})\n",
    "    results_dict.update({'AUC': myauc})\n",
    "    results_dict.update({'Opt_threshold': mythre})\n",
    "    results_dict.update({'Accuracy': accuracy})\n",
    "    results_dict.update({'Sensitivity': sensitivity})\n",
    "    results_dict.update({'Specificity': specificity})\n",
    "    results_dict.update({'Precision': precision})\n",
    "    results_dict.update({'Fall_out': fall_out})\n",
    "    results_dict.update({'Miss_rate': miss_rate})\n",
    "    results_dict.update({'F_score': F_score})\n",
    "\n",
    "    if verbose:\n",
    "        for key in [key for key in results_dict.keys() if key not in ['Y_test', 'Y_pred']]:\n",
    "            print('% s ===> %.3f' % (key, results_dict.get(key)))\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def grey_color_func(word, font_size, position, orientation, random_state=None,\n",
    "                    **kwargs):\n",
    "    return \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)\n",
    "\n",
    "\n",
    "def plot_topic_wordcloud(W, idx2word, num_keywords_in_topic=5, save_name=None, grid_shape = [2,5]):\n",
    "        # plot the class-conditioanl PMF as wordclouds \n",
    "        # W = [(p x r) (words x topic), 1 x r (regression coeff. x topic)]\n",
    "        # idx2words = list of words used in the vectorization of documents \n",
    "        # prior on class labels = empirical PMF = [ # class i examples / total ]\n",
    "        # class-conditional for class i = [ # word j in class i examples / # words in class i examples]\n",
    "             \n",
    "        beta = W[1][0,1:] # first regression coefficient is for the constant term, so omit\n",
    "        fig, axs = plt.subplots(nrows=grid_shape[0], ncols=grid_shape[1], figsize=(10, 12), subplot_kw={'xticks': [], 'yticks': []})\n",
    "        idx_topic = np.argsort(beta)\n",
    "        idx_topic = np.flip(idx_topic) \n",
    "        \n",
    "        for ax, i in zip(axs.flat, np.arange(W[0].shape[1])):\n",
    "            # dist = W[:,i]/np.sum(W[:,i])\n",
    "\n",
    "            ### Take top k keywords in each topic (top k coordinates in each column of W)\n",
    "            ### to generate text data corresponding to the ith topic, and then generate its wordcloud\n",
    "            list_words = []\n",
    "        \n",
    "            idx_keyword = np.argsort(W[0][:,idx_topic[i]])\n",
    "            idx_keyword = np.flip(idx_keyword)   \n",
    "        \n",
    "            for j in range(num_keywords_in_topic):\n",
    "                list_words.append(idx2word[idx_keyword[j]])\n",
    "                \n",
    "            Y = \" \".join(list_words)\n",
    "            #stopwords = STOPWORDS\n",
    "            wc = WordCloud(background_color=\"black\",\n",
    "                                  relative_scaling=0,\n",
    "                                  width=400,\n",
    "                                  height=400).generate(Y)\n",
    "            \n",
    "            ax.imshow(wc.recolor(color_func=grey_color_func, random_state=3),\n",
    "                                 interpolation=\"bilinear\")\n",
    "            \n",
    "            ax.set_xlabel('%1.2f' % beta[idx_topic[i]], fontsize=15)\n",
    "            ax.xaxis.set_label_coords(0.5, -0.05)\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.08)\n",
    "        if save_name is not None:\n",
    "            plt.savefig(save_name, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNMF_class = SNMF(X=[X_train, Y_train],  # data, label\n",
    "                        X_test=[X_test, Y_test],\n",
    "                        #X_auxiliary = None,\n",
    "                        n_components=25,  # =: r = number of columns in dictionary matrices W, W'\n",
    "                        # ini_loading=None,  # Initializatio for [W,W'], W1.shape = [d1, r], W2.shape = [d2, r]\n",
    "                        # ini_loading=[W_true, np.hstack((np.array([[0]]), Beta_true))],\n",
    "                        # ini_code = H_true, \n",
    "                        xi=0.001,  # weight on label reconstruction error\n",
    "                        L1_reg = [0,0,0], # L1 regularizer for code H, dictionary W[0], reg param W[1]\n",
    "                        L2_reg = [0,0,0], # L2 regularizer for code H, dictionary W[0], reg param W[1]\n",
    "                        nonnegativity=[True,True,False], # nonnegativity constraints on code H, dictionary W[0], reg params W[1]\n",
    "                        full_dim=False) # if true, dictionary is Id with full dimension --> Pure regression\n",
    "\n",
    "results_dict = SNMF_class.train_logistic(iter=200, subsample_size=None, \n",
    "                                        search_radius_const=100,\n",
    "                                        if_compute_recons_error=True, if_validate=True)\n",
    "\n",
    "W = results_dict.get('loading')\n",
    "plot_topic_wordcloud(W, idx2word=idx2word, num_keywords_in_topic=7, grid_shape=[5,5], save_name=\"fakejob_topic1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR\n",
    "clf = LogisticRegression(random_state=0, fit_intercept = True).fit(X_train.T, Y_train[0,:])\n",
    "P_pred = clf.predict_proba(X_test.T)\n",
    "results = compute_accuracy_metrics(Y_test[0], P_pred[:,1], use_opt_threshold=True, verbose=True)\n",
    "LR_AUC = results.get('AUC')\n",
    "print(clf.coef_)\n",
    "print(clf.intercept_)\n",
    "#print('Y_test[0] len', X_train.shape[0])\n",
    "#print('clf.coef_ len', clf.coef_.shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNMF_class = SNMF(X=[X_train, Y_train],  # data, label\n",
    "                        X_test=[X_test, Y_test],\n",
    "                        #X_auxiliary = None,\n",
    "                        n_components=25,  # =: r = number of columns in dictionary matrices W, W'\n",
    "                        # ini_loading=None,  # Initializatio for [W,W'], W1.shape = [d1, r], W2.shape = [d2, r]\n",
    "                        # ini_loading=[W_true, np.hstack((np.array([[0]]), Beta_true))],\n",
    "                        # ini_code = H_true, \n",
    "                        xi=0.1,  # weight on label reconstruction error\n",
    "                        L1_reg = [0,0,0], # L1 regularizer for code H, dictionary W[0], reg param W[1]\n",
    "                        L2_reg = [0,0,0], # L2 regularizer for code H, dictionary W[0], reg param W[1]\n",
    "                        nonnegativity=[True,True,False], # nonnegativity constraints on code H, dictionary W[0], reg params W[1]\n",
    "                        full_dim=False) # if true, dictionary is Id with full dimension --> Pure regression\n",
    "\n",
    "results_dict = SNMF_class.train_logistic(iter=200, subsample_size=1000, \n",
    "                                        dict_update_freq=1,\n",
    "                                        search_radius_const=1,\n",
    "                                        if_compute_recons_error=True, if_validate=True)\n",
    "\n",
    "W = results_dict.get('loading')\n",
    "plot_topic_wordcloud(W, idx2word=idx2word, num_keywords_in_topic=7, grid_shape=[5,5], save_name=\"fakejob_topic1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict.get('Precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNMF_class = SNMF(X=[X_train, Y_train],  # data, label\n",
    "                        X_test=[X_test, Y_test],\n",
    "                        #X_auxiliary = None,\n",
    "                        n_components=25,  # =: r = number of columns in dictionary matrices W, W'\n",
    "                        # ini_loading=None,  # Initializatio for [W,W'], W1.shape = [d1, r], W2.shape = [d2, r]\n",
    "                        # ini_loading=[W_true, np.hstack((np.array([[0]]), Beta_true))],\n",
    "                        # ini_code = H_true, \n",
    "                        xi=0.001,  # weight on label reconstruction error\n",
    "                        L1_reg = [0,0,0], # L1 regularizer for code H, dictionary W[0], reg param W[1]\n",
    "                        L2_reg = [0,0,0], # L2 regularizer for code H, dictionary W[0], reg param W[1]\n",
    "                        nonnegativity=[True,True,False], # nonnegativity constraints on code H, dictionary W[0], reg params W[1]\n",
    "                        full_dim=False) # if true, dictionary is Id with full dimension --> Pure regression\n",
    "\n",
    "results_dict = SNMF_class.train_logistic(iter=200, subsample_size=None, \n",
    "                                        search_radius_const=3,\n",
    "                                        if_compute_recons_error=True, if_validate=True)\n",
    "\n",
    "W1 = results_dict.get('loading')\n",
    "plot_topic_wordcloud(W1, idx2word=idx2word, num_keywords_in_topic=7, grid_shape=[5,5], save_name=\"fakejob_topic1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict.get('F_score')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colourgraphenv",
   "language": "python",
   "name": "colourgraphenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
